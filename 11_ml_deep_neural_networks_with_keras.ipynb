{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks\n",
    "\n",
    "\n",
    "To tackle a complex problem, such as detecting hundreds of types of objects in high-resolution images? You may need to train a much deeper DNN, perhaps with 10 layers or many more, each containing hundreds of neurons, linked by hundreds of thousands of connections.\n",
    "\n",
    " - Vanishing gradients/Exploding gradients\n",
    " - Reusing pretrained layers\n",
    " - Faster optimizer\n",
    " - Avoiding overfitting through regularization\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Setup\n",
    "\n",
    "Importing common modules. Using the Python3 as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 22:19:25.904880: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 22:19:26.538449: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/cuda/include\n",
      "2023-04-04 22:19:26.538499: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/cuda/include\n",
      "2023-04-04 22:19:26.538504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Invalid MIT-MAGIC-COOKIE-1 key"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "RESOURCE_DIR = os.path.join(PROJECT_ROOT_DIR, \"resource\")\n",
    "\n",
    "def save_fig(fig, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(RESOURCE_DIR, fig + \".\" + fig_extension)\n",
    "    print(\"Saving figure...\", fig)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Vanishing/Exploding gradients problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure... sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is when the gradients grow smaller and smaller, or larger and larger, \n",
    "# when flowing backward through  the DNN during training. Both of these problems \n",
    "# make lower layers very hard to train.\n",
    "# The backpropagation algorithm works by going from the output layer to the input layer, \n",
    "# propagating the error gradient along the way. Once the algorithm has computed \n",
    "# the gradient of the cost function with regard to each parameter in the network, \n",
    "# it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "# Unfortunately, gradients often get smaller and smaller as the algorithm progresses \n",
    "# down to the lower layers. As a result, the Gradient Descent update leaves the lower \n",
    "# layers’ connection weights virtually unchanged, and training never converges to a \n",
    "# good solution. This the vanishing gradients problem.\n",
    "#\n",
    "# In some cases, the opposite can happen: the gradients can grow bigger and bigger \n",
    "# until layers get insanely large weight updates and the algorithm diverges. \n",
    "# This is the exploding gradients problem,\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glorot and Bengio propose a way to significantly alleviate\n",
    "# the unstable gradients problem. They point out that we need the signal to\n",
    "# flow properly in both directions: in the forward direction when making\n",
    "# predictions, and in the reverse direction when backpropagating gradients.\n",
    "# We don’t want the signal to die out, nor do we want it to explode and\n",
    "# saturate. For the signal to flow properly, the authors argue that we need the\n",
    "# variance of the outputs of each layer to be equal to the variance of its\n",
    "# inputs, and we need the gradients to have equal variance before and after\n",
    "# flowing through a layer in the reverse direction.\n",
    "\n",
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x7f36a02d9610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialization    Activation functions                    σ² (Normal)\n",
    "# --------------    --------------------                   -------------   \n",
    "# Glorot            None, tanh, logistic, softmax            1/fan_avg\n",
    "# He                ReLU and variants                        2/fan_in\n",
    "# LeCun             SELU                                     1/fan_in\n",
    "#\n",
    "# The initialization strategy for the ReLU activation function (and its variants, \n",
    "# including the ELU activation described shortly) is sometimes called \n",
    "# He initialization. By default, Keras uses Glorot initialization with a \n",
    "# uniform distribution. When creating a layer, you can change this to He initialization\n",
    "\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x7f36a02aa8e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want He initialization with a uniform distribution but based on\n",
    "# fan avg rather than fan in , you can use the VarianceScaling initializer.\n",
    "\n",
    "\n",
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                          distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Nonsaturating Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function behaves much better in in DNN, mostly because it does not\n",
    "# saturate for positive values (and because it is fast to compute).\n",
    "#\n",
    "# Unfortunately, the ReLU activation function is not perfect. It suffers from\n",
    "# a problem known as the dying ReLUs: during training, some neurons effectively “die,” \n",
    "# meaning they stop outputting anything other than 0. In some cases, you may find that \n",
    "# half of your network’s neurons are dead, especially if you used a large learning rate. \n",
    "# A neuron dies when its weights get tweaked in such a way that the weighted sum of \n",
    "# its inputs are negative for all instances in the training set. When this happens, \n",
    "# it just keeps outputting zeros, and Gradient Descent does not affect it anymore because\n",
    "# the gradient of the ReLU function is zero when its input is negative.\n",
    "#\n",
    "# To solve this problem, you may want to use a variant of the ReLU function, such as \n",
    "# the leaky ReLU. This function is defined as LeakyReLU α (z) = max(αz, z). \n",
    "# The hyperparameter α defines how much the function “leaks”: it is the slope of the \n",
    "# function for z < 0 and is typically set to 0.01. This small slope ensures that \n",
    "# leaky ReLUs never die; they can go into a long coma, but they have a chance to\n",
    "# eventually wake up.\n",
    "\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure... leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqV0lEQVR4nO3de3xU1b3//9cHAkIICfCLoNQCxwtWwIIaqdZL02LVClYRUBQFvICXI2KPWKVeSsW7qFWs4gWKClUQUKza/hRPo4IWiYqnQgsVCwpyU0gg5kaS9f1jDTqEhMxMMtlzeT8fj3mwZ8/O3u/ZM8xn9t5r1jLnHCIiIommRdABRERE6qICJSIiCUkFSkREEpIKlIiIJCQVKBERSUgqUCIikpBUoCQiZubMbGjQOZKZmY02s5Jm2lazvF5mdoKZ/Z+ZVZpZQby310CWHqHnnRdkDmk6KlApwMxmmtkrQeeIhplNCn2YODOrMbMvzWy2mX0/yvUUmNkj9Ty21swm1LPtT2LNHmGuugrEHODgJt5Ofa/9gcCfm3Jb9XgI+Bg4BDinGbYH1Pu6f4F/3subK4fElwqUBGkV/gPlIOA84EhgbqCJ4sg5V+ac29JM29rknKtohk0dCvyvc+4L59y2ZthevZxz1aHnXRVkDmk6KlBpwMx6mdmrZrbTzLaY2XNmdkDY48ea2etm9pWZ7TCzxWZ2fAPrvCG0/Amhvxla6/Gfm9kuM+uyj9VUhT5QvnTOvQM8CRxnZtlh6znTzD4ws3Iz+4+Z3WFmrWPcFRExs5ZmNj20vTIz+7eZ/drMWtRabpSZ/cPMKsxss5k9HZq/NrTIC6EjqbWh+d+e4jOznqHHjqy1zrGh/dqqoRxmNgkYBQwMOxrNDz22xxGcmR1pZotC69kWOvLKCXt8ppm9YmbjzWyDmW03sz+aWWY9+6iHmTkgB5gR2t5oM8sPTefWXnb3qbewZQaY2VIzKzWzQjM7utY2jjOz/zWzb8ysODTd1cxmAj8B/jvsefeo6xSfmZ0c2kZ56DV6MPz9EzoSe9TM7gzt9y1mNqX2ay3B0IuQ4szsQOBt4BOgP3AKkAUsDPtP2B54FjgptMxy4DUz+//qWJ+Z2RRgHPAT59wS4DngklqLXgK84pzbHGHOA/CniKpDN8zsNGA28AjQO7TOocCdkayzEVoAG4BzgSOAm4DfABeH5b0ceBz4I/BD4Az8PgY4NvTvGPwR4u7733LOrQaWASNqPTQCmOuc2xVBjin4I85Foe0cCLxbe1tm1g74/4ES/Os7GPgxMKPWoicBffDvkfNCy42vvb6Q3afTSoFrQ9Nz6lm2PncBNwJHA18Ds83MQpn7An8DPgVOAI4LrT8jlOk9/L7f/by/qON5fw/4C/ARcBRwKXB+aLvhRgBV+H1ydej5nBflc5F4cM7pluQ3YCa+GNT12G3Am7XmdQQc0L+evzFgI3Bh2DyH/0/7R2A10D3ssTz8f/Dvha2/DBi0j8yT8IWoBP8h50K3h8KWeRu4pdbfnR36GwvdLwAeqWcba4EJ9Wz7kyj38d3AorD764G797G8A4bWmjcaKAm7fw2wLuy5dANqgB9HkaPO1z58+/hCWQy0D3s8P7TMoWHr+QJoGbbMk+HbqidPCTC6jvXmhs3rEZqXV2uZ08KWOSE076DQ/dnAe/vY7l6vex3buQP4N9Ci1mtQAWSGree9Wut5A3gq1v+PujXdTUdQqe8Y4GQzK9l947tvm4cAmFlnM3vczFabWTGwE+iM/8AMNwX/4XKic27d7pnOuULgH/jTTQAXANvw3173ZQ3QD3+EcRPwIf4IITz7TbWy/wloBxxAHJnZFaHTTltD2/0Vof1hZp2B7wFvNnIzzwNd8Ucu4L/d/8c59+1R0L5yROEI4P+cczvD5r2LL4a9wuatdM5Vh93/Ev8+iJf/q7UtwrZ3FPC/jVz/EcDfnXM1YfMWA63x187qyrE7Szyft0RIBSr1tQBexReC8NthwO7WX0/ji8Sv8Kc5+uGPEGpf63kDXxjOqGM7T+G/nYI/Ffd0rQ+7ulQ65z51zq1wzt2J/6D4Q63sv6uV+4eh7FsbWDfADvw1kto64I8o6mRm5wG/xx9VnBba7qPsvT8axfkGE2/w3Wm+Efgjh+bMET6cwa46Hov2M2J3MbCwea3qWTZ8e7tzNNdnUlM/b4mDjKADSNx9iL+Gsc756xp1ORG4xjn3KoD5hg0H1rHca8ACQhf/nXNPhz02G7jPzK7GX1MYHkPW24FVZjbVOfdBKPsPnHOfxrAu8K0Ej6lj/tGhx+pzIrDUOfdtM2YzO2T3tHNui5ltAAbgC0xddgEtI8g4C3jEzJ7At2IMb2yyzxwhlRFs55/AJWbWPuwo6sf4D+F/RpAxGru/OBwYNt0vhvV8BPxsH49H+rzPNbMWYUdRJ4b+dk0MmaSZ6VtC6sg2s361bj3wRyQ5wBwz+5GZHWxmp5jZE2bWPvS3q4ELzbf2OxZ/6qmyro04514BhgHTzGxk2Pwi4AXgfuBt59y/o30Czrk1wEJgcmjWbcAFZnabmfUxsx+Y2VAzu7fWn+bW8dy7Ag8Cp5nZLaHn1tvM7gCODz1Wn9XA0Wb2CzM7zMxuwbcaC3cHcK2Z/cp8i7x+ZnZd2ONrgQFmdoCZddzHtl7CH2FMB5Y533gimhxrgT5mdriZ5ZpZXUcrs/HX+Z4x35rvZHwDjwWNKP71+RR/CnlSaL+cCtwcw3ruA44KvU/7hp7fZWa2+/TmWqB/qOVebj2t7h7Fn0J91MyOMLOB+Gt4jzjnSmPIJM0t6ItgujX+hj8F5Oq4zQs9fhgwD9iOb7ywCpgKtA493hdYGnpsDXARvkXapLBt7HHRHzgztPzIsHknh5YbGUHmSdTRUAH/zd4RaigAnAq8g/+A3QEUAleHLV9Qz3OfUuvvt+FbihUAJzeQrTW+YGwHikLTtwJray13KbASX8w3ATNq7Z9/44+k1obmjSaskUTYss+EMl8TbQ5gf+B1/HVDB+TX83odib9mVhZa30wgp9Z76JVa26/zNaq1zB6NJMJew+Whbb0HDKTuRhL1NqQIzTsR31CmLPT8FwEHhh7rGVr37gY2PepZx8n493YFsBn/xWS/Wu+f2o0t9toXugVz2916SKTRQtdMHge6On1DFZFG0jUoaTTzP+Y8AN8C70kVJxFpCroGJU3h1/jThtv47vqRiEij6BSfiIgkJB1BiYhIQorbNajc3FzXo0ePeK2+Ub755hvatWsXdIykpf0Xm1WrVlFdXU2vXr0aXlj2ovdd7Orbd1u2wBdfgBn84AeQWWfXwPH3wQcffOWc27/2/LgVqB49elBYWBiv1TdKQUEB+fn5QcdIWtp/scnPz6eoqChh/18kOr3vYlfXvnvzTTjtND/9/PNw7rnNn2s3M1tX13yd4hMRSTOffeYLUnU1TJwYbHHaFxUoEZE0UlICZ58N27bBwIEwOYHb3apAiYikCedg9Gj4xz/g8MNh9mxoGUmPkQFRgRIRSRN33AHz50N2NixcCDl19fWfQFSgRETSwMKFcMstvsXec8/5I6hEF1WBCvWoXG5ms+IVSEREmtbatZlceKGfvvNOOKOuEd0SULRHUH8AlsUjiIiINL3t2+Hmm/tQUgLnnQc33BB0oshFXKDMbDi+y/vGDnMtIiLNoLoahg+HDRsy6dcPZszwp/iSRUQ/1DWzbPzgcT8DLtvHcmOBsQBdunShoKCgCSI2vZKSkoTNlgy0/2JTVFREdXW19l2M9L6L3rRpB/P6693Izq7ghhs+5P33K4KOFJVIe5KYDEx3zq23fZRf59wTwBMAeXl5LlF/9a1fpDeO9l9sOnToQFFRkfZdjPS+i87s2TBnDmRkwO9+t5Lhw48POlLUGixQZtYPOAU4Ku5pRESk0T74AC4Lnet66CHo1as42EAxiuQIKh8/lPLnoaOnLKClmfVyzh0dv2giIhKtzZt9TxHl5TBmDFx5Jbz1VtCpYhNJgXoCeD7s/gR8wboyHoFERCQ2lZUwZAisXw8//jE88khyNYqorcECFRq++9shvM2sBCh3zm2NZzAREYnONdfAkiXwve/5HiNatw46UeNEPdyGc25SHHKIiEgjTJsGjz8O++0HL70EBxwQdKLGU1dHIiJJ7p13YNw4P/3kk5CXF2yepqICJSKSxD7/3F93qqqC666Diy4KOlHTUYESEUlSpaW+xd7WrfDzn8PddwedqGmpQImIJCHn/G+dPvoIDjnED9ueEXWrgsSmAiUikoTuu88Pm5GV5YfS6NQp6ERNTwVKRCTJ/PWvcOONfvrZZ6F372DzxIsKlIhIElm92vdQ7hxMmuSvQaUqFSgRkSSxYwecdRYUF8PgwX6E3FSmAiUikgRqamDECPjXv6BPH3j6aWiR4p/gKf70RERSw623wiuvQMeOvqeI9u2DThR/KlAiIgnuhRfgjjv8EdPcub5ZeTpQgRIRSWAffwyjR/vpKVPglFMCjdOsVKBERBLUV1/5VnqlpTByJFx7bdCJmpcKlIhIAtq1C849F9auhWOP9T2VJ/PYTrFQgRIRSUDXXQd/+5sfNuPFF6FNm6ATNT8VKBGRBDNjBkyd6gccXLDAD0CYjlSgREQSyN//Dlde6acffRSOPz7YPEFSgRIRSRBffgnnnAOVlXD11XDppUEnCpYKlIhIAigv990XbdwI+fnwwANBJwqeCpSISMCcgyuugPffh+7d/Y9xW7UKOlXwVKBERAL28MO+b73MTN+N0f77B50oMahAiYgE6M03fZNygD/+Efr1CzROQlGBEhEJyGef+R/jVlfDb37jp+U7KlAiIgEoKfFjO23bBgMHwuTJQSdKPCpQIiLNrKbGdwD7ySdw+OEwe3bqj+0UC+0SEZFmdscdMH8+5OTAwoX+X9mbCpSISDNauNAPPmgGf/qTP4KSuqlAiYg0kxUr4MIL/fRdd8EZZwSbJ9GpQImINIPt2/3YTiUlMHw4/PrXQSdKfCpQIiJxVlXli9Knn8JRR8H06ek3tlMsVKBEROJs4kR4/XXIzfVjO2VmBp0oOahAiYjE0ezZMGUKZGTAvHm+rz2JjAqUiEicFBbCZZf56Ycfhp/8JNg8yUYFSkQkDjZv9sNnlJfDmDG+t3KJjgqUiEgTq6yEIUNg/Xo44QR45BE1ioiFCpSISBMbNw6WLIHvfc9fd2rdOuhEyUkFSkSkCU2bBk88AW3a+LGdDjgg6ETJSwVKRKSJvP22P3oCePJJyMsLNk+yU4ESEWkCn38OQ4f6H+Ved913XRpJ7CIqUGY2y8w2mtkOM1ttZpfFO5iISLIoLfXdGG3dCqeeCnffHXSi1BDpEdRdQA/nXDbwS+B2MzsmfrFERJKDc3DppfDRR3DIIfD88/5HudJ4ERUo59wK51zF7ruh2yFxSyUikiTuu88XpawsP5RGx45BJ0odEdd5M3sUGA20BT4CXqtjmbHAWIAuXbpQUFDQJCGbWklJScJmSwbaf7EpKiqiurpa+y5Gifi+W7q0ExMnHgkYN9zwD7Zu/ZoEiwgk5r6LhDnnIl/YrCVwPJAP3OOc21Xfsnl5ea6wsLDRAeOhoKCA/Pz8oGMkLe2/2OTn51NUVMTy5cuDjpKUEu19t3o19O8PxcXwu9/5QQgTVaLtu9rM7APn3F5tHqNqxeecq3bOLQYOAq5sqnAiIsmkuBjOOsv/e845cPPNQSdKTbE2M89A16BEJA3V1Pgm5P/6F/TpA08/DS30g524aHC3mllnMxtuZllm1tLMTgPOB96MfzwRkcRy663wyivQqZNvFJGVFXSi1BVJIwmHP503DV/Q1gHXOudejmcwEZFE88ILcMcd/ohpzhw4+OCgE6W2BguUc24roFFMRCStffwxjB7tp++/H045JdA4aUFnTkVEGvDVV75RRGkpjBoF48cHnSg9qECJiOzDrl0wbBisW+eblU+bprGdmosKlIjIPlx3HRQU+GEzFizww2hI81CBEhGpx4wZMHWqH3BwwQI/AKE0HxUoEZE6vPceXBnqjuCxx+D444PNk45UoEREatmwwfcQUVkJV18Nl1wSdKL0pAIlIhKmvNwXp02bID8fHngg6ETpSwVKRCTEObjiCnj/feje3f8wt1WroFOlLxUoEZGQhx7yfetlZvpujHJzg06U3lSgRESARYtgwgQ/PXMm9O0baBxBBUpEhM8+g/POg+pq+M1v/A9zJXgqUCKS1kpKfDdG27bBoEEweXLQiWQ3FSgRSVs1NTByJHzyCRx+OMyapbGdEoleChFJW7ffDi++CDk5vlFETk7QiSScCpSIpKWFC+G3v/Udvz73nD+CksSiAiUiaWfFCj9sO8Bdd8EvfhFsHqmbCpSIpJVt23yjiJISGD4cfv3roBNJfVSgRCRtVFXB+efDmjVw1FEwfbrGdkpkKlAikjYmToTXX4f994eXXvI9RkjiUoESkbQwaxZMmQIZGTBvHnTrFnQiaYgKlIikvMJCuOwyP/3ww3DyycHmkcioQIlIStu0CQYPhooKGDvW91YuyUEFSkRSVmUlDB0K69fDCSf44dvVKCJ5qECJSEpyzo+Gu2QJHHQQzJ8PrVsHnUqioQIlIilp2jR48klo08Z3Z9SlS9CJJFoqUCKSct5+G665xk8/+STk5QWbR2KjAiUiKWXdOn/dqarKD0C4u0sjST4qUCKSMkpLfYu9rVvh1FPh7ruDTiSNoQIlIinBObj0UvjoIzj0UHj+eWjZMuhU0hgqUCKSEu691xelrCzfjVHHjkEnksZSgRKRpPfaa76fPfBdGvXuHWweaRoqUCKS1Fatggsu8Kf4brvND6UhqUEFSkSSVnGxL0jFxXDOOXDTTUEnkqakAiUiSam6GkaM8EdQffrA009DC32ipRS9nCKSlG69FV59FTp1goULfeMISS0qUCKSdObOhTvv9M3I586Fgw8OOpHEgwqUiCSVjz+Giy/201OmwIABweaR+FGBEpGk8dVXvlFEaSmMGgXjxwedSOJJBUpEkkJVlTFsmO9rr39/31u5xnZKbQ0WKDPbz8ymm9k6M9tpZsvN7BfNEU5EZLdHHz2EggI44AA/fEabNkEnkniL5AgqA/gC+AmQA9wMzDWzHnHMJSLyrenT4cUXD6J1a1iwALp2DTqRNIeMhhZwzn0DTAqb9YqZ/Qc4Blgbn1giIt5778GVV/rpxx6D448PNo80nwYLVG1m1gXoCayo47GxwFiALl26UFBQ0Nh8cVFSUpKw2ZKB9l9sioqKqK6u1r6LwtatrbniimPYtWs/Bg36DwcfvA7tvugl6//ZqAqUmbUCZgNPO+f+Vftx59wTwBMAeXl5Lj8/vykyNrmCggISNVsy0P6LTYcOHSgqKtK+i1B5OZx8MmzbBj/9KYwf/7n2XYyS9f9sxK34zKwF8CxQCVwdt0Qikvacg8svh2XLoEcP/2PcjAwXdCxpZhEdQZmZAdOBLsAZzrldcU0lImntoYfgmWcgM9OP7ZSbG3QiCUKkp/geA44ATnHOlcUxj4ikuUWL4Lrr/PTMmdC3b6BxJECR/A6qO3A50A/YZGYloduIeIcTkfSyZg2cey7U1PihM4YNCzqRBCmSZubrAP1eW0TiqqQEzj4btm+HQYP84IOS3tTVkYgErqYGRo6ETz6BH/zAD9uusZ1EbwERCdztt/vui3Jy/NhOOTlBJ5JEoAIlIoF66SX47W99x6/PPw89ewadSBKFCpSIBGbFCrjoIj99991w+unB5pHEogIlIoHYts2P7VRSAuefD9dfH3QiSTQqUCLS7KqqYPhw36z8qKPgqac0tpPsTQVKRJrdjTfCG2/A/vv7a1CZmUEnkkSkAiUizerZZ+H++yEjA+bPh27dgk4kiUoFSkSaTWEhjBnjp6dOhZNOCjaPJDYVKBFpFps2+Z4iKipg7Fi44oqgE0miU4ESkbirqIAhQ2DDBjjhBH/0JNIQFSgRiSvnYNw4ePddOOggf92pdeugU0kyUIESkbiaNg2efBLatPHdGXXpEnQiSRYqUCISN2+9Bddc46efegry8oLNI8lFBUpE4mLdOhg61P8od8IEGKER5CRKKlAi0uRKS32Lva++glNP9f3siURLBUpEmpRzcMklsHw5HHqo76G8ZcugU0kyUoESkSZ1770wZw5kZfmxnTp2DDqRJCsVKBFpMq+9BhMn+unZs6FXr2DzSHJTgRKRJrFqlR82wzm47Tb45S+DTiTJTgVKRBqtuNiP7bRjh+8x4qabgk4kqUAFSkQapbraNyFftQqOPBJmzoQW+mSRJqC3kYg0yq23wquvQqdOfmynrKygE0mqUIESkZjNnQt33umbkc+dCwcfHHQiSSUqUCISk+XL4eKL/fT998OAAYHGkRSkAiUiUdu61fcUUVoKo0d/19+eSFNSgRKRqOzaBcOG+b72+veHxx4Ds6BTSSpSgRKRqPzP//heyg880A+f0aZN0IkkValAiUjEpk+HRx7xAw4uWABduwadSFKZCpSIROTdd+HKK/30tGlw3HHB5pHUpwIlIg1avx7OOcdff7rmmu9a74nEkwqUiOxTebkvTps3w09/ClOmBJ1I0oUKlIjUyzkYOxaWLYMePfyPcVu1CjqVpAsVKBGp1+9/D88+C5mZfmyn3NygE0k6UYESkTotWgQTJvjpmTPhhz8MNI6kIRUoEdnLmjVw7rlQU+OHzhg2LOhEko5UoERkDzt3+rGdtm+HM8/0gw+KBEEFSkS+VVMDo0bBihVwxBEwa5bGdpLgRPTWM7OrzazQzCrMbGacM4lIQCZP9t0X5eT4sZ2ys4NOJOksI8LlvgRuB04D2sYvjogE5aWXYNIkf8T0/PPQs2fQiSTdRVSgnHMLAMwsDzgorolEpNmtWAEXXeSn77oLTj892DwioGtQImlv2zbfKKKkBM4/H66/PuhEIl6kp/giYmZjgbEAXbp0oaCgoClX32RKSkoSNlsy0P6LTVFREdXV1Qm176qrjRtvPJI1azpx2GE7GTnyI956qyboWHXS+y52ybrvmrRAOeeeAJ4AyMvLc/n5+U25+iZTUFBAomZLBtp/senQoQNFRUUJte+uuw4KC2H//WHRovZ063Zy0JHqpfdd7JJ13+kUn0iaevZZeOAByMiA+fOhW7egE4nsKaIjKDPLCC3bEmhpZm2AKudcVTzDiUh8LFsGY8b46alT4aSTgs0jUpdIj6BuBsqAG4ELQ9M3xyuUiMTPpk0weDBUVMDll8MVVwSdSKRukTYznwRMimsSEYm7igoYMgQ2bIATT4SHHw46kUj9dA1KJE04B1df7YduP+ggmDcPWrcOOpVI/VSgRNLEY4/BU09Bmza+14guXYJOJLJvKlAiaeCtt2D8eD89fTocc0yweUQioQIlkuLWrYOhQ6GqyvcSccEFQScSiYwKlEgKKy2Fs8+Gr76C007z/eyJJAsVKJEU5RxccgksXw6HHgrPPQctWwadSiRyKlAiKeqee2DOHMjKgoULoWPHoBOJREcFSiQFvfoq/OY3fnr2bOjVK9g8IrFQgWomZsa8efOCjiFpYNUq3xDCOT9C7i9/GXQikdioQIWMHj2aQYMGBR1DpFGKi/3YTjt2+B4jbrop6EQisVOBEkkR1dUwYoQ/gjrySJg5E8yCTiUSOxWoCKxcuZKBAwfSvn17OnfuzPnnn8+mTZu+fXzZsmWceuqp5Obmkp2dzYknnsh77723z3Xec8895Obm8ve//z3e8SVN3HKLv/bUqZNvFJGVFXQikcZRgWrAxo0bOfnkk+nTpw/vv/8+ixYtoqSkhLPOOouaGj/y6M6dO7nooot45513eP/99+nXrx9nnHEGX3/99V7rc84xYcIEpk6dyltvvcVxxx3X3E9JUtCcOf43Ti1bwty58F//FXQikcZr0hF1U9Fjjz1G3759ueeee76d98wzz9CpUycKCwvp378/P/vZz/b4m6lTpzJ//nz+8pe/cOGFF347v7q6mksuuYQlS5awZMkSunfv3mzPQ1LX8uVw8cV++oEHYMCAQOOINBkVqAZ88MEHvP3222TVcb5kzZo19O/fny1btnDLLbfwt7/9jc2bN1NdXU1ZWRmff/75HstPmDCBjIwMli5dSufOnZvrKUgK27rVN4ooK4PRo2HcuKATiTQdFagG1NTUMHDgQKZMmbLXY11C3UGPGjWKzZs38+CDD9KjRw/2228/BgwYQGVl5R7L//znP+e5557jtddeY/To0c0RX1LYrl0wbBh8/jn86Ee+t3I1ipBUogLVgKOPPpq5c+fSvXt3WrVqVecyixcv5uGHH2bgwIEAbN68mY0bN+613BlnnME555zDsGHDMDNGjRoV1+yS2n71K99L+YEHwoIFfhgNkVSiRhJhduzYwfLly/e4DRw4kOLiYs477zyWLl3KZ599xqJFixg7diw7d+4EoGfPnsyaNYuVK1eybNkyhg8fTut6RoIbNGgQL7zwAldccQXPPPNMcz49SSFPPQV/+IMfcHDBAujaNehEIk1PR1Bh3nnnHY466qg95g0ZMoQlS5YwceJETj/9dMrLy+nWrRunnnoq++23HwAzZsxg7NixHHPMMXTt2pVJkyaxdevWerczaNAg5s6dy7nnngvAyJEj4/ekJOW8+y5cdZWfnjYN1BBUUpUKVMjMmTOZOXNmvY/vq5uivn37snTp0j3mXXTRRXvcd87tcf/MM8+krKws+qCS1tavh3PO8defrrnmu9Z7IqlIp/hEkkRZGQweDJs3w89+BnW02xFJKSpQIknAObj8cigshB49/A9z62mzI5IyVKBEksDvfw/PPguZmb4bo9zcoBOJxF/KF6hVq1YxY8aMoGOIxOyNN2DCBD/99NPwwx8Gm0ekuaRsIwnnHNOnT2f8+PHU1NTQsWNHBg8eHHQskaisWQPnnQc1NXDzzTB0aNCJRJpPSh5BFRUVcdZZZzF+/HhKS0spLy9n1KhRrF+/PuhoIhHbudN3Y7R9O5x5Jvzud0EnEmleKVeg3nvvPQ4//HBef/11SktLv51fWlrK4MGDv+2BXCSR1dTAyJGwYgUccQTMmgUtUu5/q8i+pcxbvrq6mkmTJjFgwAC2bNlCRUXFHo9nZGSwefNmysvLA0ooErnJk+Gll6BDB98oIjs76EQizS8lCtSGDRs4/vjjue++++r88WtmZiaDBw9m5cqVZGZmBpBQJHIvvgiTJvkjpueeg8MOCzqRSDCSvpHEwoULGTlyJKWlpVRVVe3xWIsWLWjbti2PP/44I0aMCCihSOQ++cSf2gO4+244/fRg84gEKWkLVFlZGePGjeNPf/pTvUdNBx98MC+//DL/peFFJQls2+YbRZSUwPnnf9e0XCRdJeUpvpUrV9KnT596i1Pbtm256qqr+PDDD1WcJClUVcHw4fDZZ3D00b63co3tJOkuqY6gnHNMmzaNCRMmUFZWtlcHrK1atSIrK4t58+btNQy7SCK74Qb/g9zOnf01KF0qFUmiArV9+3ZGjBjB22+/vUfz8d3atWtH//79mTt3LrnqB0aSyDPPwAMPQEYGzJsH3boFnUgkMSTFKb7FixfTs2dP3nzzTb755pu9Hm/bti2TJ0/mzTffVHGSpLJsGYwd66cfeQROOinYPCKJJKGPoKqqqpg0aRIPPPBAndea2rRpw/7778+f//xn+vbtG0BCkdht2uSHz6io8D2VX3550IlEEkugR1CVlZV8+OGHdT72xRdf8KMf/YgHH3yw3lZ6Q4YM4Z///KeKkySdigoYMgQ2bIATT4SHHw46kUjiCbRA3X///Rx77LEsW7Zsj/nz58+nd+/efPzxx3tdb2rRogVZWVnMmDGDWbNm0a5du+aMLNJozsF//7cfuv373/fXnVq3DjqVSOIJ7BTfjh07uPPOO6mpqeGss85i1apVZGRkcNVVVzF37tw6G0JkZmZy2GGHsXDhQrp37x5AapHGe/RRmD4d2rTxLfa6dAk6kUhiiugIysw6mdmLZvaNma0zswsau+F7772X6upqALZt28aQIUPo1asXc+bMqbM4tW3blnHjxlFYWKjiJEmrpCSDa6/109OnwzHHBBpHJKFFegT1B6AS6AL0A141s4+dcyti2ejXX3+9x7WliooKFi9eXOe1ptatW5OVlcX8+fPJz8+PZXMiCaGoCNaubUd1NVx/PVzQ6K95IqnNav/Yda8FzNoB24E+zrnVoXnPAhucczfW93ft27d3x9Tz9fDTTz9l48aNDQ590aJFC7Kzs+nVqxetWrXa9zOJQlFRER06dGiy9aUb7b+91dT43iDqu33zDWzZshyATp360aePeoqIlt53sUv0fffWW2994JzLqz0/kiOonkDV7uIU8jHwk9oLmtlYYCz4Xh2Kior2WtmuXbv48ssv9+oFoo51ccABB5Cbm1vnb58ao7q6us5sEplU3H81NUZ1dey3SLVqVcNBBxVRXBzHJ5OiUvF911ySdd9FUqCygB215hUD7Wsv6Jx7AngCIC8vzxUWFu61sjFjxvDpp59SWVlZ7wazs7NZvHgxRx55ZATxoldQUKDThY2QaPuvuhp27PCn0IqKoLj4u+m67teeV1zsj4Aao00byMnx4zeF33bP69gRFizIp7KyiOXLlzduY2kq0d53ySTR953VczohkgJVAtQeLi0b2BltiHXr1jFr1qx9Fif47igrXgVKEsuuXdEXlfB5O2p/fYpBu3Z1F5b67ofPy8nxBaohf/0rNPDWF5EwkRSo1UCGmR3mnPt3aF5fIOoGEhMnTtxrzKa6lJWVMXz4cFatWkXnzp2j3Yw0s/Ly2I9eioqgjkabUcvJ2XcR2Vehyc6GJrzEKSJNpMEC5Zz7xswWALeZ2WX4VnxnAT+OZkOrV6/mxRdfjKhAgf+d1JgxY1i4cGE0m5EoOecLRKRHK0VF8PnnR1NT8939iorGZWjRIvKjlbrut28PLVs2LoOIJJ5Im5lfBcwAtgBfA1dG28T8+uuvZ9euXXvN390zRE1NDeXl5Rx44IH07t2b/v37c8opp0SzibRUUwM7d0Z/Wiz8fujnaFHY84xvq1b+Gks0p8XCb+3aqUWbiOwtogLlnNsGnB3rRlasWMHLL79MVlYWAOXl5XTt2pU+ffpw7LHH0qdPH3r37s2hhx7apM3Jk0FV1Z4X+KM9VVZc7I+CGqNt2+hOi3322Yf89KdHf3u/TRsVGBFpes3S1VFWVha33347RxxxBL179+aQQw4hIyOhO1KPWGVldEcrte+XlDQ+Q/v2sV9/ycmJvh+4goIdHHFE43OLiOxLs1SJ7t27c9NNNzXHpqLi3J4X+GMpNHV0fhEVsz0LR6SnxXbPy872A92JiKSapP5oc84fgUR7Wmzjxv5UVPj7dVwWi0pGRvTNksNvWVm+kYCIiOwp0AJVU9O46y9FRbH+wDLz26nWrf0F/lh+/9KhA2Rm6vqLiEg8xK1Abd4Mt96670LTVD+wjPa02KpVSznttB9F/ANLERFpfnErUOvXw+TJDS+XnR379ZecnNh+YFlWVqYxeEREElzcClTnznDVVfsuNPqBpYiI1CduBer734ff/jZeaxcRkVSn9mMiIpKQVKBERCQhqUCJiEhCUoESEZGEpAIlIiIJSQVKREQSkgqUiIgkJBUoERFJSCpQIiKSkFSgREQkIZlr7Hjh9a3YbCuwLi4rb7xc4KugQyQx7b/Yad/FTvsudom+77o75/avPTNuBSqRmVmhcy4v6BzJSvsvdtp3sdO+i12y7jud4hMRkYSkAiUiIgkpXQvUE0EHSHLaf7HTvoud9l3sknLfpeU1KBERSXzpegQlIiIJTgVKREQSkgqUiIgkJBUowMwOM7NyM5sVdJZkYGb7mdl0M1tnZjvNbLmZ/SLoXInMzDqZ2Ytm9k1ov10QdKZkoPda00jWzzgVKO8PwLKgQySRDOAL4CdADnAzMNfMegQZKsH9AagEugAjgMfMrHewkZKC3mtNIyk/49K+QJnZcKAIeDPgKEnDOfeNc26Sc26tc67GOfcK8B/gmKCzJSIzawcMAW5xzpU45xYDLwMXBZss8em91njJ/BmX1gXKzLKB24D/CTpLMjOzLkBPYEXQWRJUT6DKObc6bN7HgI6goqT3WnSS/TMurQsUMBmY7pxbH3SQZGVmrYDZwNPOuX8FnSdBZQE7as0rBtoHkCVp6b0Wk6T+jEvZAmVmBWbm6rktNrN+wCnAgwFHTTgN7buw5VoAz+KvrVwdWODEVwJk15qXDewMIEtS0nsteqnwGZcRdIB4cc7l7+txM7sW6AF8bmbgv+W2NLNezrmj450vkTW07wDM77Tp+Iv+ZzjndsU7VxJbDWSY2WHOuX+H5vVFp6kiovdazPJJ8s+4tO3qyMwy2fNb7QT8i3mlc25rIKGSiJlNA/oBpzjnSgKOk/DM7HnAAZfh99trwI+dcypSDdB7LTap8BmXskdQDXHOlQKlu++bWQlQniwvXJDMrDtwOVABbAp9OwO43Dk3O7Bgie0qYAawBfga/yGh4tQAvddilwqfcWl7BCUiIoktZRtJiIhIclOBEhGRhKQCJSIiCUkFSkREEpIKlIiIJCQVKBERSUgqUCIikpBUoEREJCH9P15b8kEI3NI/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a neural network on Fashion MNIST using the Leaky ReLU:\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 22:19:31.949917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 22:19:31.958149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 22:19:31.958489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 22:19:31.960093: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 22:19:31.960785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 22:19:31.961152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 22:19:31.961413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 22:19:32.377411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 22:19:32.377628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 22:19:32.377794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-04 22:19:32.377939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 338 MB memory:  -> device: 0, name: Quadro RTX 3000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 22:19:32.482382: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 172480000 exceeds 10% of free system memory.\n",
      "2023-04-04 22:19:32.576713: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 172480000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 22:19:33.295463: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1e06def0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-04 22:19:33.295505: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Quadro RTX 3000, Compute Capability 7.5\n",
      "2023-04-04 22:19:33.342585: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 5s 2ms/step - loss: 1.2773 - accuracy: 0.6084 - val_loss: 0.8709 - val_accuracy: 0.7220\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7882 - accuracy: 0.7416 - val_loss: 0.7015 - val_accuracy: 0.7714\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6772 - accuracy: 0.7750 - val_loss: 0.6345 - val_accuracy: 0.7910\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.6189 - accuracy: 0.7940 - val_loss: 0.5826 - val_accuracy: 0.8096\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5814 - accuracy: 0.8057 - val_loss: 0.5515 - val_accuracy: 0.8190\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5542 - accuracy: 0.8123 - val_loss: 0.5287 - val_accuracy: 0.8236\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5338 - accuracy: 0.8188 - val_loss: 0.5097 - val_accuracy: 0.8310\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5179 - accuracy: 0.8227 - val_loss: 0.5024 - val_accuracy: 0.8330\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5052 - accuracy: 0.8259 - val_loss: 0.4849 - val_accuracy: 0.8388\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4941 - accuracy: 0.8301 - val_loss: 0.4780 - val_accuracy: 0.8390\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try PReLU:\n",
    "# The parametric leaky ReLU (PReLU), where α is authorized to be learned during \n",
    "# training (instead ofbeing a hyperparameter, it becomes a parameter that can \n",
    "# be modified by backpropagation like any other parameter). PReLU was reported to\n",
    "# strongly outperform ReLU on large image datasets, but on smaller datasets\n",
    "# it runs the risk of overfitting the training set.\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 22:20:08.651658: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 172480000 exceeds 10% of free system memory.\n",
      "2023-04-04 22:20:18.706835: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 164.49MiB (rounded to 172480000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-04-04 22:20:18.706909: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc\n",
      "2023-04-04 22:20:18.706937: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 28, Chunks in use: 26. 7.0KiB allocated for chunks. 6.5KiB in use in bin. 240B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.706957: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 4, Chunks in use: 4. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 1.6KiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.706975: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 4, Chunks in use: 4. 5.0KiB allocated for chunks. 5.0KiB in use in bin. 4.5KiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.706991: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 1, Chunks in use: 1. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 1.2KiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707010: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 3, Chunks in use: 3. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 11.7KiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707054: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 2, Chunks in use: 1. 16.2KiB allocated for chunks. 8.2KiB in use in bin. 4.9KiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707089: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 1, Chunks in use: 0. 25.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707112: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 2, Chunks in use: 1. 116.5KiB allocated for chunks. 53.8KiB in use in bin. 53.7KiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707133: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 5, Chunks in use: 3. 568.2KiB allocated for chunks. 351.8KiB in use in bin. 351.6KiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707150: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 1, Chunks in use: 0. 154.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707164: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707182: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 4, Chunks in use: 2. 3.36MiB allocated for chunks. 1.79MiB in use in bin. 1.79MiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707199: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 1. 1.73MiB allocated for chunks. 1.73MiB in use in bin. 918.8KiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707214: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707229: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707248: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 1. 14.95MiB allocated for chunks. 14.95MiB in use in bin. 14.95MiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707262: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707284: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707299: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707320: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 2, Chunks in use: 1. 317.63MiB allocated for chunks. 164.49MiB in use in bin. 164.49MiB client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707334: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 22:20:18.707353: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 164.49MiB was 128.00MiB, Chunk State: \n",
      "2023-04-04 22:20:18.707381: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 153.14MiB | Requested Size: 117.2KiB | in_use: 0 | bin_num: 19, prev:   Size: 14.95MiB | Requested Size: 14.95MiB | in_use: 1 | bin_num: -1\n",
      "2023-04-04 22:20:18.707394: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 355008512\n",
      "2023-04-04 22:20:18.707411: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000000 of size 256 next 1\n",
      "2023-04-04 22:20:18.707426: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000100 of size 1280 next 2\n",
      "2023-04-04 22:20:18.707439: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000600 of size 256 next 3\n",
      "2023-04-04 22:20:18.707451: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000700 of size 256 next 4\n",
      "2023-04-04 22:20:18.707463: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000800 of size 256 next 6\n",
      "2023-04-04 22:20:18.707475: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000900 of size 256 next 59\n",
      "2023-04-04 22:20:18.707487: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000a00 of size 256 next 29\n",
      "2023-04-04 22:20:18.707500: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000b00 of size 256 next 58\n",
      "2023-04-04 22:20:18.707511: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000c00 of size 256 next 54\n",
      "2023-04-04 22:20:18.707524: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000d00 of size 256 next 7\n",
      "2023-04-04 22:20:18.707536: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000e00 of size 256 next 5\n",
      "2023-04-04 22:20:18.707548: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496000f00 of size 256 next 35\n",
      "2023-04-04 22:20:18.707559: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001000 of size 256 next 11\n",
      "2023-04-04 22:20:18.707572: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001100 of size 256 next 12\n",
      "2023-04-04 22:20:18.707584: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001200 of size 256 next 10\n",
      "2023-04-04 22:20:18.707596: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001300 of size 256 next 16\n",
      "2023-04-04 22:20:18.707607: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001400 of size 256 next 17\n",
      "2023-04-04 22:20:18.707619: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001500 of size 256 next 15\n",
      "2023-04-04 22:20:18.707631: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f3496001600 of size 256 next 18\n",
      "2023-04-04 22:20:18.707643: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001700 of size 256 next 21\n",
      "2023-04-04 22:20:18.707655: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001800 of size 256 next 24\n",
      "2023-04-04 22:20:18.707667: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001900 of size 256 next 25\n",
      "2023-04-04 22:20:18.707681: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001a00 of size 256 next 26\n",
      "2023-04-04 22:20:18.707694: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496001b00 of size 1280 next 27\n",
      "2023-04-04 22:20:18.707706: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496002000 of size 1280 next 14\n",
      "2023-04-04 22:20:18.707719: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496002500 of size 8448 next 20\n",
      "2023-04-04 22:20:18.707735: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f3496004600 of size 55040 next 23\n",
      "2023-04-04 22:20:18.707747: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f3496011d00 of size 120064 next 34\n",
      "2023-04-04 22:20:18.707760: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f349602f200 of size 120064 next 32\n",
      "2023-04-04 22:20:18.707772: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f349604c700 of size 700672 next 33\n",
      "2023-04-04 22:20:18.707784: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34960f7800 of size 1811200 next 9\n",
      "2023-04-04 22:20:18.707799: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34962b1b00 of size 172480000 next 22\n",
      "2023-04-04 22:20:18.707811: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f34a072f100 of size 256 next 30\n",
      "2023-04-04 22:20:18.707823: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a072f200 of size 256 next 60\n",
      "2023-04-04 22:20:18.707835: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f34a072f300 of size 101632 next 36\n",
      "2023-04-04 22:20:18.707847: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0748000 of size 256 next 37\n",
      "2023-04-04 22:20:18.707859: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0748100 of size 512 next 31\n",
      "2023-04-04 22:20:18.707871: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f34a0748300 of size 8192 next 56\n",
      "2023-04-04 22:20:18.707883: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a074a300 of size 4096 next 61\n",
      "2023-04-04 22:20:18.707897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f34a074b300 of size 25856 next 39\n",
      "2023-04-04 22:20:18.707910: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0751800 of size 256 next 40\n",
      "2023-04-04 22:20:18.707922: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f34a0751900 of size 64256 next 44\n",
      "2023-04-04 22:20:18.707933: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0761400 of size 256 next 19\n",
      "2023-04-04 22:20:18.707945: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0761500 of size 256 next 45\n",
      "2023-04-04 22:20:18.707957: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0761600 of size 512 next 49\n",
      "2023-04-04 22:20:18.707969: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0761800 of size 512 next 43\n",
      "2023-04-04 22:20:18.707980: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0761a00 of size 512 next 41\n",
      "2023-04-04 22:20:18.707993: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0761c00 of size 1280 next 52\n",
      "2023-04-04 22:20:18.708005: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0762100 of size 2048 next 46\n",
      "2023-04-04 22:20:18.708018: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0762900 of size 4096 next 47\n",
      "2023-04-04 22:20:18.708030: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0763900 of size 4096 next 48\n",
      "2023-04-04 22:20:18.708041: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f34a0764900 of size 158464 next 51\n",
      "2023-04-04 22:20:18.708053: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a078b400 of size 120064 next 28\n",
      "2023-04-04 22:20:18.708066: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a07a8900 of size 120064 next 42\n",
      "2023-04-04 22:20:18.708077: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f34a07c5e00 of size 940800 next 13\n",
      "2023-04-04 22:20:18.708092: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a08ab900 of size 940800 next 50\n",
      "2023-04-04 22:20:18.708104: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0991400 of size 940800 next 53\n",
      "2023-04-04 22:20:18.708117: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f34a0a76f00 of size 15680000 next 38\n",
      "2023-04-04 22:20:18.708132: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f34a196b100 of size 160583424 next 18446744073709551615\n",
      "2023-04-04 22:20:18.708144: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-04-04 22:20:18.708160: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 26 Chunks of size 256 totalling 6.5KiB\n",
      "2023-04-04 22:20:18.708175: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 512 totalling 2.0KiB\n",
      "2023-04-04 22:20:18.708188: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 1280 totalling 5.0KiB\n",
      "2023-04-04 22:20:18.708201: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2048 totalling 2.0KiB\n",
      "2023-04-04 22:20:18.708215: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 4096 totalling 12.0KiB\n",
      "2023-04-04 22:20:18.708229: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 8448 totalling 8.2KiB\n",
      "2023-04-04 22:20:18.708243: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 55040 totalling 53.8KiB\n",
      "2023-04-04 22:20:18.708258: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 120064 totalling 351.8KiB\n",
      "2023-04-04 22:20:18.708271: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 2 Chunks of size 940800 totalling 1.79MiB\n",
      "2023-04-04 22:20:18.708285: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1811200 totalling 1.73MiB\n",
      "2023-04-04 22:20:18.708299: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 15680000 totalling 14.95MiB\n",
      "2023-04-04 22:20:18.708313: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 172480000 totalling 164.49MiB\n",
      "2023-04-04 22:20:18.708327: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 183.40MiB\n",
      "2023-04-04 22:20:18.708341: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 355008512 memory_limit_: 355008512 available bytes: 0 curr_region_allocation_bytes_: 710017024\n",
      "2023-04-04 22:20:18.708362: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                       355008512\n",
      "InUse:                       192304640\n",
      "MaxInUse:                    194058240\n",
      "NumAllocs:                      695823\n",
      "MaxAllocSize:                172480000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-04-04 22:20:18.708422: W tensorflow/tsl/framework/bfc_allocator.cc:492] *******************************************************_____________________________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### ELU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the exponential linear unit (ELU) that outperformed all the ReLU variants \n",
    "# in the authors’ experiments: training time was reduced, and the neural network performed \n",
    "# better on the test set.\n",
    "#\n",
    "# It takes on negative values when z < 0, which allows the unit to have an average \n",
    "# output closer to 0 and helps alleviate the vanishing gradients problem.\n",
    "# \n",
    "# It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n",
    "# \n",
    "# If α is equal to 1 then the function is smooth everywhere, including around z = 0, \n",
    "# which helps speed up Gradient Descent since it does not bounce as much to the \n",
    "# left and right of z = 0.\n",
    "\n",
    "\n",
    "\n",
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing ELU in TensorFlow is trivial, just specify the activation function \n",
    "# when building each layer:\n",
    "\n",
    "\n",
    "keras.layers.Dense(10, activation=\"elu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### SELU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ<sub>1</sub> or ℓ<sub>2</sub> regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SELU is easy.\n",
    "\n",
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU \n",
    "# activation function.\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train it. Do not forget to scale the inputs to mean 0 and \n",
    "# standard deviation 1.\n",
    "# Calculate the Z-Score distributions\n",
    "\n",
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at what happens if we try to use the ReLU activation function instead.\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not great at all, we suffered from the vanishing/exploding gradients problem.\n",
    "\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Batch Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.\n",
    "\n",
    "The Batch Normalization (BN) that addresses these problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer’s inputs.\n",
    "\n",
    "In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler ); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).\n",
    "\n",
    "In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bn1.updates #deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sometimes applying BN before the activation function works better (there's a debate on this topic). Moreover, the layer before a `BatchNormalization` layer does not need to have bias terms, since the `BatchNormalization` layer has some as well, it would be a waste of parameters, so you can set `use_bias=False` when creating those layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Gradient Clipping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This is called Gradient Clipping.\n",
    "\n",
    "This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs. For other types of networks,\n",
    "BN is usually sufficient.\n",
    "\n",
    "In Keras, implementing Gradient Clipping is just a matter of setting the `clipvalue` or `clipnorm` argument when creating an optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Reusing Pretrained Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing a Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then reuse the lower layers of this network. This technique is called `transfer learning`. It will not only speed up training considerably, but also require significantly less training data.\n",
    "\n",
    "_If the input pictures of your new task don’t have the same size as the ones used in\n",
    "_the original task, you will usually have to add a preprocessing step to resize them to\n",
    "_the size expected by the original model. More generally, transfer learning will work\n",
    "best when the inputs have similar low-level features._\n",
    "_The more similar the tasks are, the more layers you want to reuse (starting with the\n",
    "lower layers). For very similar tasks, try keeping all the hidden layers and just\n",
    "replacing the output layer._\n",
    "\n",
    "\n",
    "Let's split the fashion MNIST training set in two:S\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_A[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_B[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"trained_models/keras_saved_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"trained_models/keras_saved_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `model_B_on_A` and `model_A` actually share layers now, so when we train one, it will update both models. If we want to avoid that, we need to build `model_B_on_A` on top of a *clone* of `model_A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, what's the final verdict?\n",
    "\n",
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We got quite a bit of transfer: the error rate dropped by a factor of 4.9!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(100 - 97.05) / (100 - 99.40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Unsupervised Pretraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to tackle a complex task for which you don’t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. You should try to gather more labeled training data, but if you can’t, you may still be able to perform unsupervised pretraining. \n",
    "\n",
    "Indeed, it is often cheap to gather unlabeled training examples, but expensive to label them. If youcan gather plenty of unlabeled training data, you can try to use it to train an unsupervised model, such as an autoencoder or a generative adversarial network.Then you can reuse the lower layers of the autoencoder or the lower layers of the GAN’s discriminator, add the output layer for your task on top, and fine-tune the final network using supervised learning (i.e., with the labeled training examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Faster Optimizers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution):\n",
    "\n",
    "- **Applying a good initialization strategy for the connection weights**\n",
    "- **Using a good activation function**\n",
    "- **Using Batch Normalization** \n",
    "- **Reusing parts of a pretrained network**\n",
    "\n",
    "Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m (multiplied by the learning rate η), and it updates the weights by adding this momentum vector. In other words, the gradient is used for acceleration, not for speed. To simulate some sort of\n",
    "friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter β, called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.\n",
    "\n",
    "Implementing momentum optimization in Keras is a no-brainer: just use the SGD optimizer and set its momentum hyperparameter, then lie back and profit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Nesterov Accelerated Gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This measures the gradient of the costfunction not at the local position θ but slightly ahead in the direction of the momentum, at θ + βm.\n",
    "\n",
    "This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### AdaGrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the elongated bowl problem: Gradient Descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The AdaGrad algorithm 15 achieves this correction by scaling down the gradient vector along the steepest dimensions.\n",
    "\n",
    "Essentially, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum. One additional benefit is that it requires much less tuning of the learning rate hyperparameter η.\n",
    "\n",
    "_AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks.The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### RMSProp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never converging to the global optimum. The RMSProp algorithm 16 fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step.\n",
    "\n",
    "The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but this default value often works well, so you may not need to tune it at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Adam Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam, which stands for `adaptive moment estimation`, combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of pastgradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients.\n",
    "\n",
    "The momentum decay hyperparameter β 1 is typically initialized to 0.9, while the scaling decay hyperparameter β 2 is often initialized to 0.999. \n",
    "\n",
    "Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter `η`. You can often use the default value η=0.001, making Adam even easier to use than Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Adamax Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Nadam Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Learning Rate Scheduling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can find a good learning rate by training the model for a few hundred iterations, exponentially increasing the learning rate from a very small value to a very large value, and then looking at the learning curve and picking a learning rate slightly lower than the one at which the learning curve starts shooting back up. You can then reinitialize your model and train it with that learning rate.\n",
    "\n",
    "But you can do better than a constant learning rate: if you start with a large learning rate and then reduce it once training stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many different strategies to reduce the learning rate during training. It can also be beneficial to start with a low learning rate, increase it, then drop it again. These strategies are called learning schedules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Power Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lr = lr0 / (1 + steps / s)**c```\n",
    "* Keras uses `c=1` and `s = 1 / decay`\n",
    "\n",
    "The learning rate drops at each step. After s steps, the initial learning rate `η0` is down to η0/2. After s more steps, it is down to η0/3, then it goes down to η0/4, then η0/5, and so on. As you cansee, this schedule first drops quickly, then more and more slowly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay = 1e-4\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = math.ceil(len(X_train) / batch_size)\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Exponential Scheduling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```lr = lr0 * 0.1**(epoch / s)```\n",
    "\n",
    "The learning rate will gradually drop by a factor of 10 every s steps. While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every s steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schedule function can take the current learning rate as a second argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to update the learning rate at each iteration rather than at each epoch, you must write your own callback class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Note: the `batch` argument is reset at each epoch\n",
    "        lr = K.get_value(self.model.optimizer.learning_rate)\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr * 0.1**(1 / self.s))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.learning_rate)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "lr0 = 0.01\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=lr0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "exp_decay = ExponentialDecay(s)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[exp_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = n_epochs * len(X_train) // 32\n",
    "steps = np.arange(n_steps)\n",
    "lrs = lr0 * 0.1**(steps / s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(steps, lrs, \"-\", linewidth=2)\n",
    "plt.axis([0, n_steps - 1, 0, lr0 * 1.1])\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling (per batch)\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Piecewise Constant Scheduling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a constant learning rate for a number of epochs (e.g., η0=0.1 for 5 epochs), then a smaller learning rate for another number of epochs (e.g., η1 =0.001 for 50 epochs), and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Performance Scheduling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the validation error every N steps (just like for early stopping), and reduce the learning rate by a factor of λ when the error stops dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\", color='b')\n",
    "plt.tick_params('y', colors='b')\n",
    "plt.gca().set_xlim(0, n_epochs - 1)\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\n",
    "ax2.set_ylabel('Validation Loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### tf.keras schedulers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For piecewise constant scheduling, try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],\n",
    "    values=[0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### 1Cycle scheduling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: In the `on_batch_end()` method, `logs[\"loss\"]` used to contain the batch loss, but in TensorFlow 2.2.0 it was replaced with the mean loss (since the start of the epoch). This explains why the graph below is much smoother than in the book (if you are using TF 2.2 or above). It also means that there is a lag between the moment the batch loss starts exploding and the moment the explosion becomes clear in the graph. So you should choose a slightly smaller learning rate than you would have chosen with the \"noisy\" graph. Alternatively, you can tweak the `ExponentialLearningRate` callback above so it computes the batch loss (based on the current mean loss and the previous mean loss):\n",
    "\n",
    "```python\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.prev_loss = 0\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        batch_loss = logs[\"loss\"] * (batch + 1) - self.prev_loss * batch\n",
    "        self.prev_loss = logs[\"loss\"]\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(batch_loss)\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Avoiding Overfitting Through Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### $\\ell_1$ and $\\ell_2$ regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For simple linear models, you can use $\\ell_2$ regularization to constrain a neural network’s connection weights, and/or $\\ell_1$ regularization if you want a sparse model \n",
    "(with many weights equal to 0). Here is how to apply $\\ell_2$ regularization to a Keras layer’s connection weights, using a regularization factor of 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor of 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Since you will typically want to apply the same regularizer to all layers in your network, as well as using the same activation function and the same initialization strategy in all hidden layers, you may find yourself repeating the same arguments. This makes the code ugly and error-prone. To avoid this, you can try refactoring your code to use loops. Another option is touse Python’s functools.partial() function, which lets you create a thin wrapper for any callable, with some default argument values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dropout is one of the most popular regularization techniques for deep neural networks.\n",
    "Even the state-of-the-art neural networks get a 1–2% accuracy boost simply by adding dropout. This may not sound like a lot, but when a model already has 95% accuracy, getting a 2% accuracy boost means dropping the error rate by almost 40% (going from 5% error to roughly 3%).\n",
    "\n",
    "Neurons trained with dropout cannot co-adapt with their neighboring neurons, they have to be asuseful as possible on their own. They also cannot rely excessively on just a few input neurons, they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Alpha Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### MC Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model with MC Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Max norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
